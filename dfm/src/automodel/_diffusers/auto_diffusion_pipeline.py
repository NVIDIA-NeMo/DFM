# Copyright (c) 2025, NVIDIA CORPORATION. All rights reserved.
#
# Licensed under the Apache License, Version 2.0 (the "License");
# you may not use this file except in compliance with the License.
# You may obtain a copy of the License at
#
#     http://www.apache.org/licenses/LICENSE-2.0
#
# Unless required by applicable law or agreed to in writing, software
# distributed under the License is distributed on an "AS IS" BASIS,
# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
# See the License for the specific language governing permissions and
# limitations under the License.

import logging
import os
from typing import Any, Dict, Iterable, Optional, Tuple

import torch
import torch.nn as nn
from Automodel.distributed.dfm_parallelizer import WanParallelizationStrategy
from diffusers import DiffusionPipeline
from nemo_automodel.components.distributed import parallelizer
from nemo_automodel.components.distributed.fsdp2 import FSDP2Manager
from nemo_automodel.shared.utils import dtype_from_str


logger = logging.getLogger(__name__)


def _init_parallelizer():
    parallelizer.PARALLELIZATION_STRATEGIES["WanTransformer3DModel"] = WanParallelizationStrategy()


def _choose_device(device: Optional[torch.device]) -> torch.device:
    if device is not None:
        return device
    if torch.cuda.is_available():
        local_rank = int(os.environ.get("LOCAL_RANK", 0))
        return torch.device("cuda", local_rank)
    return torch.device("cpu")


def _iter_pipeline_modules(pipe: DiffusionPipeline) -> Iterable[Tuple[str, nn.Module]]:
    # Prefer Diffusers' components registry when available
    if hasattr(pipe, "components") and isinstance(pipe.components, dict):
        for name, value in pipe.components.items():
            if isinstance(value, nn.Module):
                yield name, value
        return

    # Fallback: inspect attributes
    for name in dir(pipe):
        if name.startswith("_"):
            continue
        try:
            value = getattr(pipe, name)
        except Exception:
            continue
        if isinstance(value, nn.Module):
            yield name, value


def _move_module_to_device(module: nn.Module, device: torch.device, torch_dtype: Any) -> None:
    # torch_dtype can be "auto", torch.dtype, or string
    dtype: Optional[torch.dtype]
    if torch_dtype == "auto":
        dtype = None
    else:
        dtype = dtype_from_str(torch_dtype) if isinstance(torch_dtype, str) else torch_dtype
    if dtype is not None:
        module.to(device=device, dtype=dtype)
    else:
        module.to(device=device)


def _ensure_params_trainable(module: nn.Module, module_name: Optional[str] = None) -> int:
    """
    Ensure that all parameters in the given module are trainable.

    Returns the number of parameters marked trainable. If a module name is
    provided, it will be used in the log message for clarity.
    """
    num_trainable_parameters = 0
    for parameter in module.parameters():
        parameter.requires_grad = True
        num_trainable_parameters += parameter.numel()
    if module_name is None:
        module_name = module.__class__.__name__
    logger.info("[Trainable] %s: %s parameters set requires_grad=True", module_name, f"{num_trainable_parameters:,}")
    return num_trainable_parameters


class NeMoAutoDiffusionPipeline(DiffusionPipeline):
    """
    Drop-in Diffusers pipeline that adds optional FSDP2/TP parallelization during from_pretrained.

    Features:
    - Accepts a per-component mapping from component name to FSDP2Manager init args
    - Moves all nn.Module components to the chosen device/dtype
    - Parallelizes only components present in the mapping by constructing a manager per component

    parallel_scheme:
    - Dict[str, Dict[str, Any]]: component name -> kwargs for FSDP2Manager(...)
    """

    @classmethod
    def from_pretrained(
        cls,
        pretrained_model_name_or_path: str,
        *model_args,
        parallel_scheme: Optional[Dict[str, Dict[str, Any]]] = None,
        device: Optional[torch.device] = None,
        torch_dtype: Any = "auto",
        move_to_device: bool = True,
        load_for_training: bool = False,
        components_to_load: Optional[Iterable[str]] = None,
        **kwargs,
    ) -> tuple[DiffusionPipeline, Dict[str, FSDP2Manager]]:
        pipe: DiffusionPipeline = DiffusionPipeline.from_pretrained(
            pretrained_model_name_or_path,
            *model_args,
            torch_dtype=torch_dtype,
            **kwargs,
        )
        # Decide device
        dev = _choose_device(device)

        # Move modules to device/dtype first (helps avoid initial OOM during sharding)
        if move_to_device:
            for name, module in _iter_pipeline_modules(pipe):
                if not components_to_load or name in components_to_load:
                    logger.info("[INFO] Moving module: %s to device/dtype", name)
                    _move_module_to_device(module, dev, torch_dtype)

        # If loading for training, ensure the target module parameters are trainable
        if load_for_training:
            for name, module in _iter_pipeline_modules(pipe):
                if not components_to_load or name in components_to_load:
                    logger.info("[INFO] Ensuring params trainable: %s", name)
                    _ensure_params_trainable(module, module_name=name)

        # Use per-component FSDP2Manager init-args to parallelize components
        created_managers: Dict[str, FSDP2Manager] = {}
        if parallel_scheme is not None:
            assert torch.distributed.is_initialized(), "Expect distributed environment to be initialized"
            _init_parallelizer()
            for comp_name, comp_module in _iter_pipeline_modules(pipe):
                manager_args = parallel_scheme.get(comp_name)
                if manager_args is None:
                    continue
                manager = FSDP2Manager(**manager_args)
                created_managers[comp_name] = manager
                parallel_module = manager.parallelize(comp_module)
                setattr(pipe, comp_name, parallel_module)
        return pipe, created_managers

    @classmethod
    def from_config(
        cls,
        pretrained_model_name_or_path: str,
        *model_args,
        parallel_scheme: Optional[Dict[str, Dict[str, Any]]] = None,
        device: Optional[torch.device] = None,
        torch_dtype: Any = "auto",
        move_to_device: bool = True,
        load_for_training: bool = False,
        components_to_load: Optional[Iterable[str]] = None,
        **kwargs,
    ) -> tuple[DiffusionPipeline, Dict[str, FSDP2Manager]]:
        config = WanTransformer3DModel.from_pretrained(
            pretrained_model_name_or_path,
            subfolder="transformer",
            torch_dtype=torch_dtype,
            **kwargs,
        )
        pipe: DiffusionPipeline = DiffusionPipeline.from_config(
            config,
            *model_args,
            torch_dtype=torch_dtype,
            **kwargs,
        )
        # Decide device
        dev = _choose_device(device)

        # Move modules to device/dtype first (helps avoid initial OOM during sharding)
        if move_to_device:
            for name, module in _iter_pipeline_modules(pipe):
                if not components_to_load or name in components_to_load:
                    logger.info("[INFO] Moving module: %s to device/dtype", name)
                    _move_module_to_device(module, dev, torch_dtype)

        # If loading for training, ensure the target module parameters are trainable
        if load_for_training:
            for name, module in _iter_pipeline_modules(pipe):
                if not components_to_load or name in components_to_load:
                    logger.info("[INFO] Ensuring params trainable: %s", name)
                    _ensure_params_trainable(module, module_name=name)

        # Use per-component FSDP2Manager init-args to parallelize components
        created_managers: Dict[str, FSDP2Manager] = {}
        if parallel_scheme is not None:
            assert torch.distributed.is_initialized(), "Expect distributed environment to be initialized"
            _init_parallelizer()
            for comp_name, comp_module in _iter_pipeline_modules(pipe):
                manager_args = parallel_scheme.get(comp_name)
                if manager_args is None:
                    continue
                manager = FSDP2Manager(**manager_args)
                created_managers[comp_name] = manager
                parallel_module = manager.parallelize(comp_module)
                setattr(pipe, comp_name, parallel_module)
        return pipe, created_managers
