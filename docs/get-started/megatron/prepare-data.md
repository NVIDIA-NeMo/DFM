---
description: "Prepare Smithsonian Butterflies dataset for Megatron training"
categories: ["getting-started", "megatron"]
tags: ["data-preparation", "webdataset", "how-to"]
personas: ["mle-focused"]
difficulty: "intermediate"
content_type: "how-to"
---

(gs-megatron-prepare-data)=

# Prepare Butterfly Dataset

Convert the Smithsonian Butterflies dataset from Hugging Face into webdataset format for Megatron training.

## Goal

Create a webdataset with image latents and text embeddings ready for DiT training.

**Time**: 15-30 minutes

## Prerequisites

- ✅ Complete [Installation](../installation.md)
- ✅ Multi-GPU system (recommended for parallel processing)
- ✅ ~10GB free storage for dataset
- ✅ Internet connection (to download from Hugging Face)

## Overview

**What happens during data preparation**:
1. Download Smithsonian Butterflies dataset from Hugging Face
2. Encode images to latents using Cosmos tokenizer
3. Generate T5 text embeddings from captions
4. Package into webdataset tar shards

**Dataset details**:
- **Source**: `huggan/smithsonian_butterflies_subset`
- **Images**: ~800 butterfly images
- **Captions**: Scientific names (e.g., "Morpho menelaus")
- **Output format**: Webdataset tar shards

## Step 1: Verify Dependencies

Ensure required packages are installed:

```bash
pip install pandas webdataset transformers mediapy
```

Check the preparation script exists:

```bash
ls -l examples/megatron/recipes/dit/prepare_energon_dataset_butterfly.py
```

## Step 2: Run Data Preparation

### Single GPU Preparation

Prepare the dataset on a single GPU:

```bash
cd /opt/DFM  # Or your DFM installation path

torchrun --nproc-per-node 1 \
    examples/megatron/recipes/dit/prepare_energon_dataset_butterfly.py \
    --output-dir butterfly_webdataset
```

**What this does**:
1. Downloads dataset from `hf://datasets/huggan/smithsonian_butterflies_subset`
2. Loads Cosmos-0.1-Tokenizer-CV4x8x8 (video tokenizer)
3. Loads T5-11B text encoder
4. Processes each image:
   - Resizes to 512px shortest side
   - Ensures dimensions divisible by 16
   - Encodes to latent space
   - Generates T5 embeddings from caption
5. Saves to `butterfly_webdataset/` as tar shards

### Multi-GPU Preparation (Faster)

Speed up processing using multiple GPUs:

```bash
torchrun --nproc-per-node 4 \
    examples/megatron/recipes/dit/prepare_energon_dataset_butterfly.py \
    --output-dir butterfly_webdataset
```

**Each GPU processes a subset of images in parallel.**

### Expected Output

```text
[INFO] Rank 0 of 4 processing 834 samples
[INFO] Rank 0 of 4 processing 208 samples, from 0 to 208
[INFO] Rank 1 of 4 processing 208 samples, from 208 to 416
[INFO] Rank 2 of 4 processing 209 samples, from 416 to 625
[INFO] Rank 3 of 4 processing 209 samples, from 625 to 834
100%|██████████| 208/208 [05:23<00:00,  1.55s/it]
[INFO] Webdataset shards saved to butterfly_webdataset/
```

**Processing time**:
- Single GPU: ~30 minutes
- 4 GPUs: ~8 minutes

## Step 3: Verify Dataset

Check that webdataset shards were created:

```bash
ls -lh butterfly_webdataset/
```

Expected structure:

```text
butterfly_webdataset/
  ├── rank0-000000.tar
  ├── rank1-000000.tar
  ├── rank2-000000.tar
  └── rank3-000000.tar
```

**Shard details**:
- Each tar contains ~200 samples (configured by `maxcount=10000` in script)
- Samples include: `.pth` (latents), `.pickle` (text embeddings), `.json` (metadata)

### Inspect a Sample

```python
import webdataset as wds

dataset = wds.WebDataset("butterfly_webdataset/rank0-000000.tar")
sample = next(iter(dataset))

print(sample.keys())  # ['__key__', '.pth', '.pickle', '.json']
print(sample['.json'])  # {'image_height': 512, 'image_width': 384, ...}
```

## Understanding the Data Format

### Sample Structure

Each sample in the webdataset contains:

```python
{
    "__key__": "000042",  # Sample ID
    ".pth": tensor,       # Image latent (torch.bfloat16, shape: [1, 16, T, H, W])
    ".pickle": bytes,     # Pickled T5 text embedding (torch.bfloat16, shape: [1, 512, 4096])
    ".json": {            # Metadata
        "image_height": 512,
        "image_width": 384
    }
}
```

### Latent Space

**Image latents**:
- Original image: RGB, H×W
- After Cosmos tokenizer: 16 channels, H/8 × W/8 spatial dims
- Datatype: `bfloat16` for memory efficiency

**Text embeddings**:
- Generated by T5-11B encoder
- Max length: 512 tokens
- Embedding dim: 4096

## Troubleshooting

### Out of Memory During Preparation

```
RuntimeError: CUDA out of memory
```

**Solution**: Reduce batch size or use more GPUs:

```bash
# Use more GPUs to split work
torchrun --nproc-per-node 8 \
    examples/megatron/recipes/dit/prepare_energon_dataset_butterfly.py \
    --output-dir butterfly_webdataset
```

### T5 Model Download Fails

**Solution**: Set cache directory and verify connection:

```bash
export HF_HOME=/path/to/cache
export TRANSFORMERS_CACHE=/path/to/cache

# Test connection
python -c "from transformers import T5EncoderModel; T5EncoderModel.from_pretrained('google-t5/t5-11b')"
```

### Cosmos Tokenizer Error

```
FileNotFoundError: Cosmos-0.1-Tokenizer-CV4x8x8 not found
```

**Solution**: Download tokenizer explicitly:

```python
from nemo.collections.common.video_tokenizers.cosmos_tokenizer import CausalVideoTokenizer

tokenizer = CausalVideoTokenizer.from_pretrained("Cosmos-0.1-Tokenizer-CV4x8x8")
```

### Slow Processing

**Expected speeds**:
- Single GPU: ~2-3 images/second
- 4 GPUs: ~8-10 images/second

**Speed up**:
1. Use more GPUs for parallel processing
2. Use faster storage (SSD vs. HDD)
3. Increase `num_workers` in script (edit line 20)

## Using Your Own Dataset

### Requirements

To adapt this script for your dataset:

1. **Data format**: Images with text captions
2. **Access**: Load via pandas DataFrame
3. **Structure**: Columns for `image_url` and `caption`

### Example: Custom Dataset

```python
# In prepare_energon_dataset_butterfly.py, replace line 53:

# Original:
# df = pd.read_parquet("hf://datasets/huggan/smithsonian_butterflies_subset/data/train-00000-of-00001.parquet")

# Custom dataset:
df = pd.read_csv("/path/to/your/dataset.csv")
# Ensure df has columns: image_url, caption
```

Then run preparation as normal.

## Next Steps

After preparing your dataset:

1. **[Train DiT model](training.md)**: Use your webdataset for training
2. **Verify data loading**: Check that training loads shards correctly
3. **Scale up**: Prepare larger datasets using the same workflow

## Related Pages

- **[Megatron Training](training.md)**: Train on your prepared dataset
- **[Video Data Concepts](../../about/concepts/video-data.md)**: Understand data formats
- **[Data Loading Reference](../../reference/data-loading.md)**: Advanced data pipeline

